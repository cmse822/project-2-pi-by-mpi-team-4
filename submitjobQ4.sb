#!/bin/bash
########## Define Resources Needed with SBATCH Lines ##########
 
#SBATCH --time=01:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --ntasks-per-node=16                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --nodes=4
#SBATCH --cpus-per-task=1           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem=100M                    # memory required per node - amount of memory (in bytes)
#SBATCH --job-name Proj2Q3Attempt1  # you can give your job a name for easier identification (same as -J)
 

 
 
########## Command Lines to Run ##########

module purge
module load GCC/7.3.0-2.30 OpenMPI HDF5 Python git
  
cd /mnt/home/gerlac37/project-2-pi-by-mpi-team-4/    ### change to the directory where your code is located

for darts in 1E3 1E6 1E9; do
    for procs in 1 2 4 8 16 32 64; do
        mpiexec -n $procs Q3vars.out $darts
    done
done

# scontrol show job $SLURM_JOB_ID     ### write job information to output file
